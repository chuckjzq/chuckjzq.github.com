<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记性越来越差，只能把学过的东西记下来，不然白学"><title>tensorflow-CNN卷积神经网络 | 老姜工作室</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">tensorflow-CNN卷积神经网络</h1><a id="logo" href="/.">老姜工作室</a><p class="description">数据挖掘 | 编程 | 学习笔记</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">tensorflow-CNN卷积神经网络</h1><div class="post-meta">Oct 7, 2017<span> | </span><span class="category"><a href="/categories/深度学习/">深度学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="post-content"><h1 id="tensorflow-CNN卷积神经网络"><a href="#tensorflow-CNN卷积神经网络" class="headerlink" title="tensorflow-CNN卷积神经网络"></a>tensorflow-CNN卷积神经网络</h1><h2 id="卷积神经网络简介"><a href="#卷积神经网络简介" class="headerlink" title="卷积神经网络简介"></a>卷积神经网络简介</h2><p>卷积神经网络（Convolutional Neutral Network, CNN）是一种利用卷积核来提取特征的一种神经网络，因为他具有多层神经网络结构所以隶属于深度神经网络之中。它具有提取<strong>空间结构特征</strong>的作用，现在不仅仅用在图像像素特征的提取，而且还可以时间序列信号，例如音频和文本数据。</p>
<p>卷积神经网络中的几个概念：</p>
<ul>
<li>卷积核：卷积核的作用是用来提取特征，每一种卷积核只用来提取<strong>一种</strong>特征（例如，边或角），所以如果要表达一个物体我们需要多个特征，即需要定义多个卷积核。</li>
<li>权值共享：每一个卷积层有多个卷积核，每个卷积核对一副图像进行卷积得到一种卷积后的图像，所以每个卷积层会输出多个卷积后的图像，那么每个卷积核的权值是共享的。假设一副图像有<code>n*n</code>个像素，假设卷积核是<code>5*5</code>，那么就有<code>5*5*(n-5+1)*(n-5+1)</code>个连接，但是因为权值是共享的，所以我们所要求的参数只有<code>5*5</code>个。</li>
<li>池化层：池化层是用来对卷积后图像像素进行降采样的，例如对卷积后的图像缩小一半。</li>
<li>正则化：防止过拟合，L1正则化会制造稀疏的特征，大部分无用的特征会被置为0；L2正则化会让特征的权重不过大，使特征的权重不过大。</li>
</ul>
<p>根据‘感受野’这个概念，每个感受野只接受一小块区域的信号，一小块像素是互相关联的，但是太远的像素之间就不具有很强的关联。所以每个神经元不需要接受所以的像素点，而只需要接受局部的像素点。</p>
<p>每个卷积核产生的特征最后只会提取出一个值，即最后的特征值。然后可以把这些卷积产生的特征值输入到全连接神经网络中进行预测。</p>
<h2 id="tensorflow实现简单卷积神经网络"><a href="#tensorflow实现简单卷积神经网络" class="headerlink" title="tensorflow实现简单卷积神经网络"></a>tensorflow实现简单卷积神经网络</h2><p>几个概念：</p>
<ul>
<li>最大池化：最大池化会保留原始像素块中灰度值最大的那个像素。</li>
<li>tf.nn.conv2d:tensorflow中的2维卷积函数</li>
<li>tf.nn.max_pool: tensorflow中的最大池化函数</li>
</ul>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">2</span>]: <span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"></div><div class="line">In [<span class="number">3</span>]: <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">In [<span class="number">4</span>]: mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</div><div class="line">Extracting MNIST_data/train-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/train-labels-idx1-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</div><div class="line">In [<span class="number">5</span>]: sess = tf.InteractiveSession()</div><div class="line">In [<span class="number">9</span>]: <span class="function"><span class="keyword">def</span>  <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">    ...:     initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">    ...:     <span class="keyword">return</span> tf.Variable(initial)</div><div class="line">    ...: <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">    ...:     initial = tf.constant(<span class="number">0.1</span>, shape=shape)</div><div class="line">    ...:     <span class="keyword">return</span> tf.Variable(initial)</div><div class="line">    ...: <span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">    ...:     <span class="keyword">return</span> tf.nn.conv2d(x,W, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    ...: <span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></div><div class="line">    ...:     <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padd</div><div class="line">    ...: ing=<span class="string">'SAME'</span>)</div><div class="line"><span class="comment"># 设置数据输入的placeholder</span></div><div class="line">In [<span class="number">10</span>]: x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line"><span class="comment"># 数据label的placeholder，placeholder相当于一个数据的入口</span></div><div class="line">In [<span class="number">11</span>]: y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line"><span class="comment"># 将一维数据转换成2维，-1表示输入的图像个数不一定</span></div><div class="line">In [<span class="number">12</span>]: x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</div><div class="line"><span class="comment"># 卷积核的参数，第一个卷积层有32个核</span></div><div class="line">In [<span class="number">13</span>]: W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>])</div><div class="line"><span class="comment"># 每个卷积核有一个偏移量，所以有32个</span></div><div class="line">In [<span class="number">14</span>]: b_conv1 = bias_variable([<span class="number">32</span>])</div><div class="line"><span class="comment"># 每个卷积核经过relu激活函数输出</span></div><div class="line">In [<span class="number">15</span>]: h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line"><span class="comment"># 经过池化层降维</span></div><div class="line">In [<span class="number">18</span>]: h_pool1 = max_pool_2x2(h_conv1)</div><div class="line"><span class="comment"># 定义第二层卷积核参数，一共有32个卷积图像输入（这里的32是每种卷积核会输出一个卷积后的图像，所以一副图像经过32个卷积核卷积后会有32幅图像），然后定义了64个卷积核。</span></div><div class="line">In [<span class="number">19</span>]: W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>])</div><div class="line">In [<span class="number">20</span>]: b_conv2 = bias_variable([<span class="number">64</span>])</div><div class="line"><span class="comment"># 卷积后激活函数输出</span></div><div class="line">In [<span class="number">23</span>]: h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line"><span class="comment"># 池化层降维</span></div><div class="line">In [<span class="number">24</span>]: h_pool2 = max_pool_2x2(h_conv2)</div><div class="line"><span class="comment"># 全连接层预测，把图像降维成7*7就不继续处理了，隐含层节点为1024个</span></div><div class="line">In [<span class="number">25</span>]: W_fc1 = weight_variable([<span class="number">7</span>*<span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</div><div class="line">In [<span class="number">26</span>]: b_fc1 = bias_variable([<span class="number">1024</span>])</div><div class="line"><span class="comment"># 把二维转换成一维，给全连接层预测</span></div><div class="line">In [<span class="number">27</span>]: h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>,<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</div><div class="line"></div><div class="line">In [<span class="number">28</span>]: h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div><div class="line"><span class="comment"># drop 率</span></div><div class="line">In [<span class="number">29</span>]: keep_prob = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">In [<span class="number">30</span>]: h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</div><div class="line"><span class="comment"># 输出层</span></div><div class="line">In [<span class="number">31</span>]: W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</div><div class="line"></div><div class="line">In [<span class="number">32</span>]: b_fc2 = bias_variable([<span class="number">10</span>])</div><div class="line"><span class="comment"># 输出激活函数softmax</span></div><div class="line">In [<span class="number">33</span>]: y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</div><div class="line"><span class="comment"># 损失函数</span></div><div class="line">In [<span class="number">34</span>]: cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), </div><div class="line">    ...: reduction_indices = [<span class="number">1</span>]))</div><div class="line"><span class="comment"># 损失函数优化算法</span></div><div class="line">In [<span class="number">35</span>]: train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</div><div class="line"><span class="comment"># 预测</span></div><div class="line">In [<span class="number">36</span>]: correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</div><div class="line">    ...: </div><div class="line"></div><div class="line">In [<span class="number">37</span>]: accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line"></div><div class="line">In [<span class="number">38</span>]: tf.global_variables_initializer().run()</div><div class="line"></div><div class="line">In [<span class="number">39</span>]: <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</div><div class="line">    ...:     batch = mnist.train.next_batch(<span class="number">50</span>)</div><div class="line">    ...:     <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    ...:         train_accuracy = accuracy.eval(feed_dict=&#123;x:batch[<span class="number">0</span>], y_: b</div><div class="line">    ...: atch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</div><div class="line">    ...:         print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy)</div><div class="line">    ...: )</div><div class="line">    ...:     train_step.run(feed_dict=&#123;x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: </div><div class="line">    ...: <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line">step <span class="number">0</span>, training accuracy <span class="number">0.12</span></div><div class="line">step <span class="number">100</span>, training accuracy <span class="number">0.82</span></div><div class="line">step <span class="number">200</span>, training accuracy <span class="number">0.94</span></div><div class="line">step <span class="number">300</span>, training accuracy <span class="number">0.9</span></div><div class="line">step <span class="number">400</span>, training accuracy <span class="number">0.98</span></div><div class="line">step <span class="number">500</span>, training accuracy <span class="number">0.98</span></div><div class="line">step <span class="number">600</span>, training accuracy <span class="number">0.96</span></div><div class="line">step <span class="number">700</span>, training accuracy <span class="number">1</span></div><div class="line">step <span class="number">800</span>, training accuracy <span class="number">0.96</span></div><div class="line">step <span class="number">900</span>, training accuracy <span class="number">0.96</span></div><div class="line">step <span class="number">1000</span>, training accuracy <span class="number">0.94</span></div><div class="line">step <span class="number">1100</span>, training accuracy <span class="number">0.92</span></div><div class="line">step <span class="number">1200</span>, training accuracy <span class="number">0.98</span></div><div class="line">step <span class="number">1300</span>, training accuracy <span class="number">0.96</span></div><div class="line">step <span class="number">1400</span>, training accuracy <span class="number">1</span></div><div class="line">step <span class="number">1500</span>, training accuracy <span class="number">0.96</span></div><div class="line">step <span class="number">1600</span>, training accuracy <span class="number">1</span></div><div class="line">step <span class="number">1700</span>, training accuracy <span class="number">0.98</span></div><div class="line">step <span class="number">1800</span>, training accuracy <span class="number">0.98</span></div><div class="line">step <span class="number">1900</span>, training accuracy <span class="number">0.96</span></div><div class="line">。。。</div><div class="line">。。。</div><div class="line"></div><div class="line">。。。</div></pre></td></tr></table></figure>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><ol>
<li><a href="http://blog.csdn.net/mao_xiao_feng/article/details/53453926" target="_blank" rel="external">【TensorFlow】tf.nn.max_pool实现池化操作</a></li>
<li><a href="http://blog.csdn.net/lxg0807/article/details/53021859" target="_blank" rel="external">TensorFlow的reshape操作 tf.reshape</a></li>
</ol>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://zhiqiang.studio/2017/10/07/tensorflow-cnn/" data-id="cjfgu71eq0071e6hwmdcucdvq" class="article-share-link">分享</a><div class="tags"><a href="/tags/深度学习/">深度学习</a></div><div class="post-nav"><a href="/2017/10/09/tensorflow-rnn/" class="pre">tensorflow-RNNs循环神经网络</a><a href="/2017/09/29/tensorflow-autoencoder/" class="next">tensorflow-自编码器</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Leetcode刷题/">Leetcode刷题</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Matlab/">Matlab</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/R语言/">R语言</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/matplotlib/">matplotlib</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/sklearn/">sklearn</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据分析/">数据分析</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/">自然语言处理</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/easy/" style="font-size: 15px;">easy</a> <a href="/tags/BeautifulSoup/" style="font-size: 15px;">BeautifulSoup</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/Hbase/" style="font-size: 15px;">Hbase</a> <a href="/tags/可视化/" style="font-size: 15px;">可视化</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/R语言/" style="font-size: 15px;">R语言</a> <a href="/tags/监督学习/" style="font-size: 15px;">监督学习</a> <a href="/tags/无监督/" style="font-size: 15px;">无监督</a> <a href="/tags/数据挖掘/" style="font-size: 15px;">数据挖掘</a> <a href="/tags/工作流程/" style="font-size: 15px;">工作流程</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/word2vec/" style="font-size: 15px;">word2vec</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/sql/" style="font-size: 15px;">sql</a> <a href="/tags/matplotlib/" style="font-size: 15px;">matplotlib</a> <a href="/tags/Mysql/" style="font-size: 15px;">Mysql</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/numpy/" style="font-size: 15px;">numpy</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/http/" style="font-size: 15px;">http</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/how-to-learn-machine-learning/">how to learn machine learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/18/gradient-vanishing/">梯度弥散(gradient-vanishing)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/07/jupyter-key/">jupyter-快捷键</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/05/HMM-model/">隐马尔可夫模型-HMM</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/01/PCA/">PCA</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/01/chunk-read-python/">chunk-read-python</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/20/sklearn-model-save/">sklearn-model-save</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/19/sklearn-SGD/">sklearn-实现随机梯度下降</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/16/datamining-flow/">datamining-flow</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/15/PRML-notes/">PRML-notes</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://blog.csdn.net/u012925804" title="My CSDN" target="_blank">My CSDN</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">老姜工作室.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>