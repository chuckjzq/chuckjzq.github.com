<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记性越来越差，只能把学过的东西记下来，不然白学"><title>tensorflow-自编码器 | 老姜工作室</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">tensorflow-自编码器</h1><a id="logo" href="/.">老姜工作室</a><p class="description">数据挖掘 | 编程 | 学习笔记</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">tensorflow-自编码器</h1><div class="post-meta">Sep 29, 2017<span> | </span><span class="category"><a href="/categories/深度学习/">深度学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="post-content"><h1 id="tensorflow-自编码器"><a href="#tensorflow-自编码器" class="headerlink" title="tensorflow-自编码器"></a>tensorflow-自编码器</h1><p>自编码器是一种输入层和输出层相同的一种网络，最简单是包含三层的一种网络，输入层、隐藏层和输出层，隐藏层的神经元的个数要少于输入层的神经元的个数，原始特征经过隐藏层输出了更高阶的特征，然后再经过特征转换到输出层和输入层接近的特征，这样一来一回就使得隐藏层输出的特征是比原始特征更加高级的特征。利用最小化损失函数和方向传播让输入层接近与输出层。</p>
<p>利用上述的理解，可以发现深度学习则可以解决人工难以提取有效特征的问题，可以大大缓解机器学习模型对特征工程的依赖。</p>
<p><strong>深度学习</strong>在早期一度被认为是一种无监督的特征学习，<strong>1. 无监督学习</strong>，即不需要标注数据就可以对数据进行一定程度的学习，这种学习是对数据内容的组织形式的学习，提取的是频繁出现的特征；<strong>2.是逐层抽象</strong>，特征是需要不断抽象的，就像人总是从简单基础的概念喀什学习，再到复杂的概念。</p>
<p><strong>稀疏编码（Sparse Coding）</strong>，简单的来说，某些高阶特征或预测结构，往往只需要<strong>一少部分</strong>特征的正交的边组合得到。也可以说是特征的<strong>稀疏表达</strong>，使用少量的基本特征组合拼装得到更高层抽象的特征。</p>
<p><strong>所以拥有多层隐藏层的神经网络，对于每一层隐藏层，前一层的输出都是未加工的像素，而这一层则是对像素进行加工组织成更高阶的特征。</strong></p>
<p>TensorFlow实战中是这么解释自编码器的：<strong>自编码器，顾名思义，即可以使用自身的高阶特征编码自己</strong>（即隐藏层的输出作为特征经过激活函数到输出层得到和输入接近的值）。它借助稀疏编码的思想，目标是使用稀疏的一些高阶特征重新组合来重构自己。它有两个特点：一、期望输入/输出一致；二、希望使用高阶特征来重构自己，而不只是复制像素点。</p>
<p>还有一点就是，<strong>自编码器可以用来初始化深度神经网络的参数</strong>，用无监督的逐层训练提取特征，将网络的权值初始化到一个比较好的位置，辅助后面的监督训练。</p>
<p>去噪自编码器。</p>
<h2 id="例子-自编码器"><a href="#例子-自编码器" class="headerlink" title="例子-自编码器"></a>例子-自编码器</h2><figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">21</span>]: import numpy as np</div><div class="line">In [<span class="number">22</span>]: import sklearn.preprocessing as prep</div><div class="line">In [<span class="number">23</span>]: import tensorflow as tf</div><div class="line">In [<span class="number">24</span>]: from tensorflow.examples.tutorials.mnist import input_data</div><div class="line">In [<span class="number">26</span>]: <span class="function"><span class="keyword">def</span> <span class="title">xavier_init</span><span class="params">(fan_in, fan_out, constant =<span class="number">1</span>)</span></span>:</div><div class="line">    ...:     low = -constant * np.sqrt(<span class="number">6.0</span> / (fan_in + fan_out))</div><div class="line">    ...:     high = constant * np.sqrt(<span class="number">6.0</span> / (fan_in + fan_out))</div><div class="line">    ...:     <span class="keyword">return</span> tf.random_uniform((fan_in, fan_out), minval = low, maxval = high, dtype = tf.float32)</div><div class="line">    ...: </div><div class="line"></div><div class="line">In [<span class="number">41</span>]: <span class="class"><span class="keyword">class</span> <span class="title">AdditiveGaussianNoiseAutoencoder</span>(<span class="title">object</span>):</span></div><div class="line">    ...:     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, n_input, n_hidden, transfer_function = tf.nn.softplus, optimizer = tf.train.AdamOptimizer()</span></span>, scale=<span class="number">0</span>.<span class="number">1</span>):</div><div class="line">    ...:         <span class="keyword">self</span>.n_input = n_input</div><div class="line">    ...:         <span class="keyword">self</span>.n_hidden = n_hidden</div><div class="line">    ...:         <span class="keyword">self</span>.transfer = transfer_function</div><div class="line">    ...:         <span class="keyword">self</span>.scale = tf.placeholder(tf.float32)</div><div class="line">    ...:         <span class="keyword">self</span>.training_scale = scale</div><div class="line">    ...:         network_weights = <span class="keyword">self</span>._initialize_weights()</div><div class="line">    ...:         <span class="keyword">self</span>.weights = network_weights</div><div class="line">    ...:         <span class="keyword">self</span>.x = tf.placeholder(tf.float32, [None, <span class="keyword">self</span>.n_input])</div><div class="line">    ...:         <span class="keyword">self</span>.hidden = <span class="keyword">self</span>.transfer(tf.add(tf.matmul(<span class="keyword">self</span>.x+ scale * tf.random_normal((n_input,)),<span class="keyword">self</span>.weights[<span class="string">'w1'</span>]),<span class="keyword">self</span>.weights[<span class="string">'b1'</span>]))</div><div class="line">    ...:         <span class="keyword">self</span>.reconstruction = tf.add(tf.matmul(<span class="keyword">self</span>.hidden, <span class="keyword">self</span>.weights[<span class="string">'w2'</span>]),<span class="keyword">self</span>.weights[<span class="string">'b2'</span>])</div><div class="line">    ...:         <span class="keyword">self</span>.cost = <span class="number">0</span>.<span class="number">5</span> * tf.reduce_sum(tf.pow(tf.subtract(<span class="keyword">self</span>.reconstruction, <span class="keyword">self</span>.x), <span class="number">2.0</span>))</div><div class="line">    ...:         <span class="keyword">self</span>.optimizer = optimizer.minimize(<span class="keyword">self</span>.cost)</div><div class="line">    ...:         init = tf.global_variables_initializer()</div><div class="line">    ...:         <span class="keyword">self</span>.sess=tf.Session()</div><div class="line">    ...:         <span class="keyword">self</span>.sess.run(init)</div><div class="line">    ...:     <span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(<span class="keyword">self</span>)</span></span>:</div><div class="line">    ...:         all_weights = dict()</div><div class="line">    ...:         all_weights[<span class="string">'w1'</span>] = tf.Variable(xavier_init(<span class="keyword">self</span>.n_input, <span class="keyword">self</span>.n_hidden))</div><div class="line">    ...:         all_weights[<span class="string">'b1'</span>] = tf.Variable(tf.zeros([<span class="keyword">self</span>.n_hidden],dtype = tf.float32))</div><div class="line">    ...:         all_weights[<span class="string">'w2'</span>] = tf.Variable(tf.zeros([<span class="keyword">self</span>.n_hidden, <span class="keyword">self</span>.n_input], dtype = tf.float32))</div><div class="line">    ...:         all_weights[<span class="string">'b2'</span>] = tf.Variable(tf.zeros([<span class="keyword">self</span>.n_input], dtype = tf.float32))</div><div class="line">    ...:         <span class="keyword">return</span> all_weights</div><div class="line">    ...:     <span class="function"><span class="keyword">def</span> <span class="title">partial_fit</span><span class="params">(<span class="keyword">self</span>, X)</span></span>:</div><div class="line">    ...:         cost, opt = <span class="keyword">self</span>.sess.run((<span class="keyword">self</span>.cost, <span class="keyword">self</span>.optimizer), feed_dict = &#123;<span class="keyword">self</span>.<span class="symbol">x:</span> X, <span class="keyword">self</span>.<span class="symbol">scale:</span> <span class="keyword">self</span>.training_scale&#125;)</div><div class="line">    ...:         <span class="keyword">return</span> cost</div><div class="line">    ...:     <span class="function"><span class="keyword">def</span> <span class="title">calc_total_cost</span><span class="params">(<span class="keyword">self</span>, X)</span></span>:</div><div class="line">    ...:         <span class="keyword">return</span> <span class="keyword">self</span>.sess.run(<span class="keyword">self</span>.cost, feed_dict = &#123;<span class="keyword">self</span>.<span class="symbol">x:</span> X, <span class="keyword">self</span>.<span class="symbol">scale:</span> <span class="keyword">self</span>.training_scale&#125;)</div><div class="line">    ...:     <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(<span class="keyword">self</span>, X)</span></span>:</div><div class="line">    ...:         <span class="keyword">return</span> <span class="keyword">self</span>.sess.run(<span class="keyword">self</span>.hidden, feed_dict = &#123;<span class="keyword">self</span>.<span class="symbol">x:</span> X, <span class="keyword">self</span>.<span class="symbol">scale:</span> <span class="keyword">self</span>.training_scale&#125;)</div><div class="line">    ...:     <span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(<span class="keyword">self</span>, hidden= None)</span></span>:</div><div class="line">    ...:         <span class="keyword">if</span> hidden is <span class="symbol">None:</span></div><div class="line">    ...:             hidden = np.random.normal(size = <span class="keyword">self</span>.weights[<span class="string">"b1"</span>])</div><div class="line">    ...:         <span class="keyword">return</span> <span class="keyword">self</span>.sess.run(<span class="keyword">self</span>.reconstruction, feed_dict = &#123;<span class="keyword">self</span>.<span class="symbol">hidden:</span> hidden&#125;)</div><div class="line">    ...:     <span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(<span class="keyword">self</span>, X)</span></span>:</div><div class="line">    ...:         <span class="keyword">return</span> <span class="keyword">self</span>.sess.run(<span class="keyword">self</span>.reconstruction, feed_dict = &#123;<span class="keyword">self</span>.<span class="symbol">x:</span> X, <span class="keyword">self</span>.<span class="symbol">scale:</span> <span class="keyword">self</span>.traing_scale&#125;)</div><div class="line">    ...:     <span class="function"><span class="keyword">def</span> <span class="title">getWeights</span><span class="params">(<span class="keyword">self</span>)</span></span>:</div><div class="line">    ...:         <span class="keyword">return</span> <span class="keyword">self</span>.sess.run(<span class="keyword">self</span>.weights[<span class="string">'w1'</span>])</div><div class="line">    ...:     <span class="function"><span class="keyword">def</span> <span class="title">getBiases</span><span class="params">(<span class="keyword">self</span>)</span></span>:</div><div class="line">    ...:         <span class="keyword">return</span> <span class="keyword">self</span>.sess.run(<span class="keyword">self</span>.weights[<span class="string">'b1'</span>])</div><div class="line">    ...: <span class="function"><span class="keyword">def</span> <span class="title">standard_scale</span><span class="params">(X_train, X_test)</span></span>:</div><div class="line">    ...:     preprocessor = prep.StandardScaler().fit(X_train)</div><div class="line">    ...:     X_train = preprocessor.transform(X_train)</div><div class="line">    ...:     X_test = preprocessor.transform(X_test)</div><div class="line">    ...:     <span class="keyword">return</span> X_train, X_test</div><div class="line">    ...: <span class="function"><span class="keyword">def</span> <span class="title">get_random_block_from_data</span><span class="params">(data, batch_size)</span></span>:</div><div class="line">    ...:     start_index = np.random.randint(<span class="number">0</span>, len(data) - batch_size)</div><div class="line">    ...:     <span class="keyword">return</span> data[<span class="symbol">start_index:</span>(start_index + batch_size)]</div><div class="line">    ...: </div><div class="line"></div><div class="line">In [<span class="number">42</span>]: autoencoder = AdditiveGaussianNoiseAutoencoder(n_input = <span class="number">784</span>, n_hidden = <span class="number">200</span>, transfer_function=tf.nn.softplus, optimizer=tf.train.AdamOptimizer(learning_rate= <span class="number">0</span>.<span class="number">001</span>), scale = <span class="number">0</span>.<span class="number">01</span>)</div><div class="line"></div><div class="line">In [<span class="number">43</span>]: <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</div><div class="line">    ...:     avg_cost = <span class="number">0</span></div><div class="line">    ...:     total_batch = int(n_samples / batch_size)</div><div class="line">    ...:     <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</div><div class="line">    ...:         batch_xs = get_random_block_from_data(X_train, batch_size)</div><div class="line">    ...:         cost = autoencoder.partial_fit(batch_xs)</div><div class="line">    ...:         avg_cost += cost/ n_samples* batch_size</div><div class="line">    ...:     <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</div><div class="line">    ...:         print(<span class="string">"Epoch:"</span>,<span class="string">'%04d'</span> % (epoch+<span class="number">1</span>),<span class="string">"cost="</span>, <span class="string">"&#123;:,.9f&#125;"</span>.format(avg_cost))</div><div class="line">    ...:         </div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0001'</span>, <span class="string">'cost='</span>, <span class="string">'19,032.752310227'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0002'</span>, <span class="string">'cost='</span>, <span class="string">'12,067.475442045'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0003'</span>, <span class="string">'cost='</span>, <span class="string">'10,731.452246591'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0004'</span>, <span class="string">'cost='</span>, <span class="string">'10,130.186838636'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0005'</span>, <span class="string">'cost='</span>, <span class="string">'9,759.724129545'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0006'</span>, <span class="string">'cost='</span>, <span class="string">'10,361.542051136'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0007'</span>, <span class="string">'cost='</span>, <span class="string">'8,671.333534659'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0008'</span>, <span class="string">'cost='</span>, <span class="string">'8,551.772588068'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0009'</span>, <span class="string">'cost='</span>, <span class="string">'8,401.630428977'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0010'</span>, <span class="string">'cost='</span>, <span class="string">'9,332.394571591'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0011'</span>, <span class="string">'cost='</span>, <span class="string">'9,359.961297727'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0012'</span>, <span class="string">'cost='</span>, <span class="string">'8,366.616878977'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0013'</span>, <span class="string">'cost='</span>, <span class="string">'8,285.494419886'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0014'</span>, <span class="string">'cost='</span>, <span class="string">'8,237.795309091'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0015'</span>, <span class="string">'cost='</span>, <span class="string">'7,824.103817614'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0016'</span>, <span class="string">'cost='</span>, <span class="string">'8,533.639329545'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0017'</span>, <span class="string">'cost='</span>, <span class="string">'8,073.315536364'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0018'</span>, <span class="string">'cost='</span>, <span class="string">'7,810.996635795'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0019'</span>, <span class="string">'cost='</span>, <span class="string">'7,771.821609091'</span>)</div><div class="line">(<span class="string">'Epoch:'</span>, <span class="string">'0020'</span>, <span class="string">'cost='</span>, <span class="string">'7,927.474592045'</span>)</div><div class="line"></div><div class="line">In [<span class="number">44</span>]: print(<span class="string">"Total cost:"</span> + str(autoencoder.calc_total_cost(X_test)))</div><div class="line">Total <span class="symbol">cost:</span><span class="number">630098.0</span></div></pre></td></tr></table></figure>
<h2 id="例子-多层感知机（三层的神经网络）"><a href="#例子-多层感知机（三层的神经网络）" class="headerlink" title="例子-多层感知机（三层的神经网络）"></a>例子-多层感知机（三层的神经网络）</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">21</span>]: import numpy <span class="keyword">as</span> np</div><div class="line">In [<span class="number">22</span>]: import sklearn.preprocessing <span class="keyword">as</span> prep</div><div class="line">In [<span class="number">23</span>]: import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></div><div class="line">In [<span class="number">24</span>]: from tensorflow.examples.tutorials.mnist import input_data</div><div class="line"></div><div class="line">In [<span class="number">46</span>]: sess = <span class="keyword">tf</span>.InteractiveSession()</div><div class="line"></div><div class="line">In [<span class="number">47</span>]: in_units = <span class="number">784</span></div><div class="line"></div><div class="line">In [<span class="number">48</span>]: h1_units = <span class="number">300</span></div><div class="line"></div><div class="line">In [<span class="number">49</span>]: w1 = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.truncated_normal([in_units,h1_units], stddev = <span class="number">0.1</span>))</div><div class="line"></div><div class="line">In [<span class="number">50</span>]: b1 = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.zeros([h1_units]))</div><div class="line"></div><div class="line">In [<span class="number">51</span>]: w2 = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.zeros([h1_units, <span class="number">10</span>]))</div><div class="line"></div><div class="line">In [<span class="number">52</span>]: b2 = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.zeros([<span class="number">10</span>]))</div><div class="line"></div><div class="line">In [<span class="number">53</span>]: <span class="keyword">x</span> = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32, [None, in_units])</div><div class="line"></div><div class="line">In [<span class="number">54</span>]: keep_prob = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32)</div><div class="line"></div><div class="line">In [<span class="number">56</span>]: hidden1 = <span class="keyword">tf</span>.<span class="keyword">nn</span>.relu(<span class="keyword">tf</span>.matmul(<span class="keyword">x</span>,w1) + b1)</div><div class="line"></div><div class="line">In [<span class="number">57</span>]: hidden1_drop = <span class="keyword">tf</span>.<span class="keyword">nn</span>.dropout(hidden1, keep_prob)</div><div class="line"></div><div class="line">In [<span class="number">58</span>]: <span class="keyword">y</span> = <span class="keyword">tf</span>.<span class="keyword">nn</span>.softmax(<span class="keyword">tf</span>.matmul(hidden1_drop, w2)+b2)</div><div class="line"></div><div class="line">In [<span class="number">59</span>]: y_ = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32, [None, <span class="number">10</span>])</div><div class="line"></div><div class="line">In [<span class="number">60</span>]: cross_entropy = <span class="keyword">tf</span>.reduce_mean(-<span class="keyword">tf</span>.reduce_sum(y_*<span class="keyword">tf</span>.<span class="built_in">log</span>(<span class="keyword">y</span>), reduction_indices=[<span class="number">1</span>]))</div><div class="line"></div><div class="line">In [<span class="number">61</span>]: train_step = <span class="keyword">tf</span>.train.AdagradOptimizer(<span class="number">0.3</span>).minimize(cross_entropy)</div><div class="line"></div><div class="line">In [<span class="number">62</span>]: <span class="keyword">tf</span>.global_variables_initializer().run()</div><div class="line"></div><div class="line">In [<span class="number">63</span>]: <span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="number">3000</span>):</div><div class="line">    ...:     batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">    ...:     train_step.run(&#123;<span class="keyword">x</span>: batch_xs, y_: batch_ys, keep_pro<span class="variable">b:</span> <span class="number">0.75</span>&#125;)</div><div class="line">    ...:     </div><div class="line">In [<span class="number">65</span>]: correct_prediction = <span class="keyword">tf</span>.equal(<span class="keyword">tf</span>.argmax(<span class="keyword">y</span>, <span class="number">1</span>), <span class="keyword">tf</span>.argmax(y_ ,<span class="number">1</span>))</div><div class="line"></div><div class="line">In [<span class="number">66</span>]: accuracy = <span class="keyword">tf</span>.reduce_mean(<span class="keyword">tf</span>.cast(correct_prediction, <span class="keyword">tf</span>.float32))</div><div class="line"></div><div class="line">In [<span class="number">67</span>]: <span class="keyword">print</span>(accuracy.<span class="built_in">eval</span>(&#123;<span class="keyword">x</span>: mnist.test.images, y_: mnist.test.labels, keep_pro<span class="variable">b:</span> <span class="number">1.0</span>&#125;))</div><div class="line"><span class="number">0.9781</span></div></pre></td></tr></table></figure>
<h2 id="注解"><a href="#注解" class="headerlink" title="注解"></a>注解</h2><ul>
<li>如果输入是什么shape的向量，那么输出也是什么形状的向量。</li>
<li>ReLU: 线性整流函数（Rectified Linear Unit, ReLU）,又称修正线性单元,<strong>f(x) = max(0,x)</strong>这个函数称为线性修正函数，采用了线性修正函数(rectified linear function)作为激活函数(activation function)的单元称为线性修正单元(rectified linear unit)，它的一个平滑解析函数为f(x)=ln(1+e^x)被称之为softplus function,softplus的微分就是logistic function(厉害)，另外一种函数叫做softmax function或者normalized exponential是logistic function的一个泛化。</li>
<li>梯度弥散问题：梯度下降法（以及相关的L-BFGS算法等）在使用随机初始化权重的深度网络上效果不好的技术原因是：梯度会变得非常小。具体而言，当使用反向传播方法计算导数的时候，随着网络的深度的增加，反向传播的梯度（从输出层到网络的最初几层）的幅度值会急剧地减小。结果就造成了整体的损失函数相对于最初几层的权重的导数非常小。这样，当使用梯度下降法的时候，最初几层的权重变化非常缓慢，以至于它们不能够从样本中进行有效的学习。这种问题通常被称为“梯度的弥散”.</li>
</ul>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><ol>
<li><a href="https://tensorflow.google.cn/api_docs/python/tf/Session?hl=zh-cn#run" target="_blank" rel="external">https://tensorflow.google.cn/api_docs/python/tf/Session?hl=zh-cn#run</a></li>
<li>TensorFlow实战第四章。</li>
<li><a href="http://www.cnblogs.com/neopenx/p/4453161.html" target="_blank" rel="external">ReLu(Rectified Linear Units)激活函数</a></li>
<li><a href="http://blog.csdn.net/lg1259156776/article/details/48379321" target="_blank" rel="external">修正线性单元（Rectified linear unit，ReLU）</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0" target="_blank" rel="external">线性整流函数</a> </li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E6%A6%82%E8%A7%88" target="_blank" rel="external">深度网络概览</a></li>
</ol>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://zhiqiang.studio/2017/09/29/tensorflow-autoencoder/" data-id="cjfgu71el006ue6hwu5wlgais" class="article-share-link">分享</a><div class="tags"><a href="/tags/深度学习/">深度学习</a></div><div class="post-nav"><a href="/2017/10/07/tensorflow-cnn/" class="pre">tensorflow-CNN卷积神经网络</a><a href="/2017/09/28/tensorflow-learning1/" class="next">tensorflow-learning1</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Leetcode刷题/">Leetcode刷题</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Matlab/">Matlab</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/R语言/">R语言</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/matplotlib/">matplotlib</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/sklearn/">sklearn</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据分析/">数据分析</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/">自然语言处理</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/easy/" style="font-size: 15px;">easy</a> <a href="/tags/BeautifulSoup/" style="font-size: 15px;">BeautifulSoup</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/Hbase/" style="font-size: 15px;">Hbase</a> <a href="/tags/可视化/" style="font-size: 15px;">可视化</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/R语言/" style="font-size: 15px;">R语言</a> <a href="/tags/监督学习/" style="font-size: 15px;">监督学习</a> <a href="/tags/无监督/" style="font-size: 15px;">无监督</a> <a href="/tags/数据挖掘/" style="font-size: 15px;">数据挖掘</a> <a href="/tags/工作流程/" style="font-size: 15px;">工作流程</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/word2vec/" style="font-size: 15px;">word2vec</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/sql/" style="font-size: 15px;">sql</a> <a href="/tags/matplotlib/" style="font-size: 15px;">matplotlib</a> <a href="/tags/Mysql/" style="font-size: 15px;">Mysql</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/numpy/" style="font-size: 15px;">numpy</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/http/" style="font-size: 15px;">http</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/how-to-learn-machine-learning/">how to learn machine learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/18/gradient-vanishing/">梯度弥散(gradient-vanishing)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/07/jupyter-key/">jupyter-快捷键</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/05/HMM-model/">隐马尔可夫模型-HMM</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/01/PCA/">PCA</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/01/chunk-read-python/">chunk-read-python</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/20/sklearn-model-save/">sklearn-model-save</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/19/sklearn-SGD/">sklearn-实现随机梯度下降</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/16/datamining-flow/">datamining-flow</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/15/PRML-notes/">PRML-notes</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://blog.csdn.net/u012925804" title="My CSDN" target="_blank">My CSDN</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">老姜工作室.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>