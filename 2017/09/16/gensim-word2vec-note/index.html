<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记性越来越差，只能把学过的东西记下来，不然白学"><title>gensim-word2vec学习笔记 | 老姜工作室</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">gensim-word2vec学习笔记</h1><a id="logo" href="/.">老姜工作室</a><p class="description">数据挖掘 | 编程 | 学习笔记</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">gensim-word2vec学习笔记</h1><div class="post-meta">Sep 16, 2017<span> | </span><span class="category"><a href="/categories/自然语言处理/">自然语言处理</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="post-content"><h1 id="gensim-word2vec学习笔记"><a href="#gensim-word2vec学习笔记" class="headerlink" title="gensim-word2vec学习笔记"></a>gensim-word2vec学习笔记</h1><h2 id="词向量和资料库"><a href="#词向量和资料库" class="headerlink" title="词向量和资料库"></a>词向量和资料库</h2><h3 id="构建流程："><a href="#构建流程：" class="headerlink" title="构建流程："></a>构建流程：</h3><ul>
<li>首先从所有的文档中抽取出主要的词作为特征，以这些词作为特征构建特征向量。</li>
<li><p>然后利用corpora.doc2bow来创建一段话的词向量，以稀疏的形式存储。</p>
<p>具体步骤如下：</p>
</li>
</ul>
<h3 id="From-Strings-to-Vectors-从字符串构建向量"><a href="#From-Strings-to-Vectors-从字符串构建向量" class="headerlink" title="From Strings to Vectors 从字符串构建向量"></a>From Strings to Vectors 从字符串构建向量</h3><h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>由于英文和中文不同，英文中可以使用空格来分割句子，但是中文中则不行。</p>
<p>所以我使用jieba来进行对句子进行分词。</p>
<p>给定一个字符串列表，即由多个字符串组成。然后<br>利用列表解析来进行把分割出的字符存储到列表中。并且可以定义一个停止列表stoplist来过滤不需要的关键词</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#停用词列表</span></div><div class="line">stoplist = set([<span class="string">'1'</span>,<span class="string">'，'</span>,<span class="string">'。'</span>])</div><div class="line"><span class="keyword">print</span> stoplist</div><div class="line"><span class="comment">#分割关键词</span></div><div class="line">texts = [[word <span class="keyword">for</span> word <span class="keyword">in</span> jieba.cut(document) <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stoplist] </div><div class="line">         <span class="keyword">for</span> document <span class="keyword">in</span> documents]</div><div class="line">         <span class="comment">#移除只出现一次的词语</span></div><div class="line">frequency = defaultdict(int)</div><div class="line"></div><div class="line"><span class="keyword">for</span> text <span class="keyword">in</span> texts:</div><div class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> text:</div><div class="line">       frequency[token] +=<span class="number">1</span></div><div class="line">texts = [[token <span class="keyword">for</span> token <span class="keyword">in</span> text <span class="keyword">if</span> frequency[token] &gt; <span class="number">1</span>] <span class="keyword">for</span> text  <span class="keyword">in</span> texts]</div><div class="line">dictionary = corpora.Dictionary(texts)</div><div class="line">dictionary.save(<span class="string">'test.dict'</span>)</div><div class="line">print(dictionary.token2id)</div><div class="line"><span class="comment">#使用doc2bow对将新的句子转换为向量。</span></div><div class="line">new_vec = dictionary.doc2bow(jieba.cut(comment_list2[<span class="number">6</span>], cut_all=<span class="keyword">False</span>))</div><div class="line"><span class="keyword">print</span> new_vec</div><div class="line"><span class="comment">#保存生成的向量特征字典</span></div><div class="line">dictionary.save(<span class="string">'test.dict'</span>)</div><div class="line"><span class="comment">#将原始的用来构造向量的句子以向量形式存储下来</span></div><div class="line">corpus = [dictionary.doc2bow(text) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</div><div class="line">corpora.MmCorpus.serialize(<span class="string">'test.mm'</span>,corpus)</div><div class="line">``` </div><div class="line"></div><div class="line">上面这些都是直接存储在内存中的，如果想一个个读取可以使用生成器generator，用<span class="keyword">yield</span>函数。</div><div class="line"></div><div class="line"><span class="comment">## 利用gensim构建模型</span></div><div class="line"><span class="number">1.</span> 初始化模型</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">## TF-IDF</span></div><div class="line"></div><div class="line">* <span class="string">"词频"</span>（Term Frequency，缩写为TF</div><div class="line">* <span class="string">"逆文档频率"</span>（Inverse Document Frequency，缩写为IDF）</div><div class="line">* 知道了<span class="string">"词频"</span>（TF）和<span class="string">"逆文档频率"</span>（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。</div><div class="line"></div><div class="line">计算流程：</div><div class="line"></div><div class="line"><span class="number">1.</span> 计算词频：`TF=某个词在文章中出现的次数/文章的总词数`</div><div class="line"></div><div class="line"><span class="number">2.</span> 计算逆文档频率：需要一个语料库（corpus），用来模拟语言的使用环境：`IDF=log(语料库的文档总数/(包含该词的文档数+<span class="number">1</span>))`</div><div class="line">如果一个词越常见，那么分母就越大，逆文档频率就越小越接近<span class="number">0</span>。分母之所以要加<span class="number">1</span>，是为了避免分母为<span class="number">0</span>（即所有文档都不包含该词）。log表示对得到的值取对数。</div><div class="line"><span class="number">3.</span> `TF-IDF=TF * IDF`可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比,所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。</div><div class="line"></div><div class="line"><span class="comment">## 余弦相似性</span></div><div class="line"></div><div class="line">基本思路是：如果这两句话的用词越相似，它们的内容就应该越相似。因此，可以从词频入手，计算它们的相似程度。</div><div class="line"></div><div class="line">* 第一步，分词。</div><div class="line">* 第二步，列出所有的词。</div><div class="line">* 第三步，计算词频。</div><div class="line">* 第四步，写出词频向量。</div><div class="line">* 第五步，计算这两个向量的相似程度。余弦值越接近<span class="number">1</span>，就表明夹角越接近<span class="number">0</span>度，也就是两个向量越相似，这就叫<span class="string">"余弦相似性"</span>。</div><div class="line"></div><div class="line">针对特定的问题，可以不需要所有出现过的词来表示文章，所以可以过滤掉一些停用词，然后来统计词频。</div><div class="line">对于很长的文章来说可以使用下面这个流程来做：</div></pre></td></tr></table></figure>
<p>   （1）使用TF-IDF算法，找出两篇文章的关键词；<br>   （2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；<br>　　（3）生成两篇文章各自的词频向量；<br>　　（4）计算两个向量的余弦相似度，值越大就表示越相似。<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="section">## 中文文本聚类</span></div><div class="line">流程如下：</div><div class="line"></div><div class="line"><span class="bullet">1. </span>分词</div><div class="line"><span class="bullet">2. </span>去除停用词</div><div class="line"><span class="bullet">3. </span>构建词袋空间</div><div class="line"><span class="bullet">4. </span>将单词出现次数转化为权重TF-IDF，这样有一个好处就是避免由于常用词过多造成两个文本主题本来不相同的句子变得很相同。</div><div class="line"><span class="bullet">5. </span>用余弦距离的k-means算法进行对文本聚类:可使用nltk库</div><div class="line"></div><div class="line"><span class="section">## nltk 使用余弦距离kmeans聚类</span></div></pre></td></tr></table></figure></p>
<p>kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)<br>assigned_clusters = kclusterer.cluster(data, assign_clusters=True)<br>```</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p><a href="http://www.cnblogs.com/dkblog/archive/2011/03/25/1995537.html" target="_blank" rel="external">python os.path模块</a><br><a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="external">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a><br><a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html" target="_blank" rel="external">TF-IDF与余弦相似性的应用（二）：找出相似文章</a><br><a href="http://blog.csdn.net/yyxyyx10/article/details/63685382" target="_blank" rel="external">python进行中文文本聚类（切词以及Kmeans聚类）</a><br><a href="http://www.nltk.org/" target="_blank" rel="external">NLTK</a></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://zhiqiang.studio/2017/09/16/gensim-word2vec-note/" data-id="cjha5krww001t4jhwsb71ni3y" class="article-share-link">分享</a><div class="tags"><a href="/tags/word2vec/">word2vec</a></div><div class="post-nav"><a href="/2017/09/19/logging-python/" class="pre">logging-python记录日志</a><a href="/2017/09/16/jieba-note/" class="next">jieba分词python</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Leetcode刷题/">Leetcode刷题</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Matlab/">Matlab</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/R语言/">R语言</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/matplotlib/">matplotlib</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/py/">py</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/sklearn/">sklearn</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据分析/">数据分析</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/自然语言处理/">自然语言处理</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/easy/" style="font-size: 15px;">easy</a> <a href="/tags/可视化/" style="font-size: 15px;">可视化</a> <a href="/tags/Matlab/" style="font-size: 15px;">Matlab</a> <a href="/tags/Hbase/" style="font-size: 15px;">Hbase</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/无监督/" style="font-size: 15px;">无监督</a> <a href="/tags/R语言/" style="font-size: 15px;">R语言</a> <a href="/tags/监督学习/" style="font-size: 15px;">监督学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/数据挖掘/" style="font-size: 15px;">数据挖掘</a> <a href="/tags/工作流程/" style="font-size: 15px;">工作流程</a> <a href="/tags/py/" style="font-size: 15px;">py</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/word2vec/" style="font-size: 15px;">word2vec</a> <a href="/tags/BeautifulSoup/" style="font-size: 15px;">BeautifulSoup</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/sql/" style="font-size: 15px;">sql</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/matplotlib/" style="font-size: 15px;">matplotlib</a> <a href="/tags/Mysql/" style="font-size: 15px;">Mysql</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/numpy/" style="font-size: 15px;">numpy</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/http/" style="font-size: 15px;">http</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/16/begining-gan/">Generative Adversarial Nets 对抗生成网络初探</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/16/semantic-segmentation-papers/">语义分割论文</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/11/git-branch-request/">gitlab分支管理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/11/python-unittest/">python-unittest</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/10/tensorflow-model-restore-and-transfer/">TensorFlow的模型恢复与迁移</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/09/python-Multiprocessing/">python-Multiprocessing</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/07/update-cudnn/">升级cudnn版本</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/how-to-learn-machine-learning/">how to learn machine learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/18/gradient-vanishing/">梯度弥散(gradient-vanishing)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/07/jupyter-key/">jupyter-快捷键</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://blog.csdn.net/u012925804" title="My CSDN" target="_blank">My CSDN</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">老姜工作室.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>